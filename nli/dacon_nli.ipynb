{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon_nli.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl-sF-wzAMm2"
      },
      "source": [
        "!pip install adamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "MmY8aXBBbyp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_mgYeWWSbysx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AdamW, RobertaForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from tqdm.notebook import tqdm, tqdm_notebook\n",
        "\n",
        "import random\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from adamp import AdamP"
      ],
      "metadata": {
        "id": "vMv1rZ2Wbyvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dacon/nli/train_data.csv')\n",
        "train_2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dacon/nli/plus_data.csv')\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dacon/nli/test_data.csv\")\n",
        "submission = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dacon/nli/sample_submission.csv\")\n"
      ],
      "metadata": {
        "id": "YCA8hRQvbyyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.concat([train_1, train_2])"
      ],
      "metadata": {
        "id": "gYzyvQ5Mby1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [(train['label']== \"entailment\"), (train['label']== \"contradiction\"), (train['label']== \"neutral\")]\n",
        "choicelist1 = [0,1,2]\n",
        "train['label']=np.select(list1, choicelist1)\n",
        "\n",
        "train=train[['premise','hypothesis','label']]\n",
        "test=test[['premise','hypothesis']]"
      ],
      "metadata": {
        "id": "4FoodjsMby31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TRAINDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    self.dataset = data\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
        "\n",
        "    print(self.dataset)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    row = self.dataset.iloc[idx, 0:3].values\n",
        "    sentence1 = row[0]\n",
        "    sentence2 = row[1]\n",
        "    y = row[2]\n",
        "    inputs = self.tokenizer(\n",
        "        sentence1,\n",
        "        sentence2,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=100\n",
        "    )\n",
        "    \n",
        "    input_ids = torch.from_numpy(np.asarray(inputs['input_ids']))\n",
        "    attention_mask = torch.from_numpy(np.asarray(inputs['attention_mask']))\n",
        "\n",
        "    return input_ids, attention_mask, y"
      ],
      "metadata": {
        "id": "TzFn3gv1by6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TESTDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    self.dataset = data\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
        "\n",
        "    print(self.dataset)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    row = self.dataset.iloc[idx, 0:2].values\n",
        "    sentence1 = row[0]\n",
        "    sentence2 = row[1]\n",
        "    inputs = self.tokenizer(\n",
        "        sentence1,\n",
        "        sentence2,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=100\n",
        "    )\n",
        "    \n",
        "    input_ids = torch.from_numpy(np.asarray(inputs['input_ids']))\n",
        "    attention_mask = torch.from_numpy(np.asarray(inputs['attention_mask']))\n",
        "\n",
        "    return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "HGSRusn3by9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "wb9LWvzebzAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "Rb_yXRzrbzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터\n",
        "epochs = 20\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "4TL0V65jbzF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 및 검증\n",
        "def training(train_dataset,val_dataset, fold):\n",
        "  best_acc = 0\n",
        "  \n",
        "  model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-large\", num_labels=3).to(device)\n",
        "  \n",
        "  dataset_train = TRAINDataset(train_dataset)\n",
        "  dataset_val = TRAINDataset(val_dataset)\n",
        "\n",
        "  train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  valid_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  optimizer = AdamP(model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "\n",
        "  total_steps = len(train_loader) * epochs\n",
        "\n",
        "  # 스케줄러\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0,\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, attention_masks, label) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "      optimizer.zero_grad()\n",
        "      token_ids = token_ids.to(device)\n",
        "      attention_masks = attention_masks.to(device)\n",
        "      label = label.to(device)\n",
        "      out = model(token_ids, attention_masks)[0]\n",
        "      loss = F.cross_entropy(out, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      train_acc += calc_accuracy(out, label)\n",
        "\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, attention_masks, label) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
        "      token_ids = token_ids.to(device)\n",
        "      attention_masks = attention_masks.to(device)\n",
        "      label = label.to(device)\n",
        "      out = model(token_ids, attention_masks)[0]\n",
        "      valid_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} valid acc {}\".format(e+1, valid_acc / (batch_id+1)))\n",
        "    torch.save(model, '/content/drive/MyDrive/Colab Notebooks/dacon/nli/model'+str(fold)+'.pt')"
      ],
      "metadata": {
        "id": "x83xR3cpbzI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차검증\n",
        "def main():\n",
        "    seed= 2021 # 재현성을 위한 시드값 고정\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
        "\n",
        "    # kfold\n",
        "    kfold=[]\n",
        "\n",
        "    splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "    for train_idx, val_idx in splitter.split(train.iloc[:, 0:2],train.iloc[:, 2]):\n",
        "        kfold.append((train.iloc[train_idx,:],train.iloc[val_idx,:]))\n",
        "\n",
        "    for fold,(train_datasets, valid_datasets) in enumerate(kfold):\n",
        "        print(f'fold{fold} 학습중...')\n",
        "        training(train_dataset=train_datasets,val_dataset=valid_datasets,fold=fold)"
      ],
      "metadata": {
        "id": "ueuGD5uhbzL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main() "
      ],
      "metadata": {
        "id": "9Hdkg2XibzO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 \n",
        "def inference(model, dataset_test):\n",
        "    test_dataset = TESTDataset(dataset_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "    output_pred = []\n",
        "    with torch.no_grad():\n",
        "      for batch_id, (token_ids, attention_masks) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        attention_masks = attention_masks.long().to(device)\n",
        "        output=model(token_ids, attention_masks)[0]\n",
        "        logits = torch.nn.functional.softmax(output, dim=1).detach().cpu().numpy()\n",
        "        output_pred.extend(logits)\n",
        "    return output_pred"
      ],
      "metadata": {
        "id": "MaYUwfVsbzR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
      ],
      "metadata": {
        "id": "Z8f9hbmObzU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 도출\n",
        "def inference_main():\n",
        "  res = np.zeros((len(test),3)) \n",
        "  for i in range(5): \n",
        "    print(f'fold{i} 모델 추론중...')\n",
        "    # load my model\n",
        "    model = torch.load('/content/drive/MyDrive/Colab Notebooks/dacon/nli/model'+str(i)+'.pt')\n",
        "\n",
        "    pred_answer = inference(model, test)\n",
        "\n",
        "    res += np.array(pred_answer) / 5 \n",
        "\n",
        "  ans= np.argmax(res, axis=-1)\n",
        "  out = [list(label_dict.keys())[_] for _ in ans]\n",
        "  submission[\"label\"] = out"
      ],
      "metadata": {
        "id": "FyoktD3ybzX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_main()"
      ],
      "metadata": {
        "id": "R0zMWGM-bza8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"FOLD5(20)_submission.csv\", index = False)"
      ],
      "metadata": {
        "id": "5eoF3A9DbzeQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}